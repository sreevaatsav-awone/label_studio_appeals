{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import pandas as pd \n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import concurrent.futures\n",
    "import re\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "client = OpenAI()\n",
    "def get_response(prompt):\n",
    "  response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages= [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }],\n",
    "    response_format={ \"type\": \"json_object\"},\n",
    "    temperature = 0\n",
    "  )\n",
    "  return json.loads(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "def get_table_df(tab_dict):\n",
    "\n",
    "    max_rows, max_cols = tab_dict[\"row_count\"], tab_dict[\"column_count\"]\n",
    "\n",
    "    table_list = [[\" \" for j in range(max_cols)] for i in range(max_rows)]\n",
    "\n",
    "    for cell in tab_dict[\"cells\"]:\n",
    "\n",
    "        column_index = cell[\"column_index\"]\n",
    "        row_index = cell[\"row_index\"]\n",
    "\n",
    "        table_list[row_index][column_index] = cell[\"content\"]\n",
    "\n",
    "    df_table = pd.DataFrame(table_list)\n",
    "    df_table = df_table.to_string(index=False)\n",
    "\n",
    "    return '<table> '+df_table +' </table>'\n",
    "\n",
    "def perform_ocr(image_data):\n",
    "    '''Function to perform OCR'''\n",
    "    endpoint = os.environ[\"AZURE_OCR_ENDPOINT\"]\n",
    "    key = os.environ[\"AZURE_OCR_KEY\"]\n",
    "    credential = AzureKeyCredential(key)\n",
    "    document_analysis = DocumentAnalysisClient(endpoint = endpoint,\n",
    "                                            credential = credential)\n",
    "    poller = document_analysis.begin_analyze_document(\"prebuilt-layout\", image_data)\n",
    "    result = poller.result()\n",
    "    return result\n",
    "\n",
    "\n",
    "def binary_search(arr, target):\n",
    "    left = 0\n",
    "    right = len(arr) - 1\n",
    "    index = -1\n",
    "\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "\n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "\n",
    "        elif arr[mid] > target:\n",
    "            index = mid\n",
    "            right = mid - 1\n",
    "\n",
    "        else:\n",
    "            left = mid + 1\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "def get_text(text_feilds):\n",
    "\n",
    "    string = \"\"\n",
    "\n",
    "    for i in range(len(text_feilds)):\n",
    "\n",
    "        if text_feilds[i][\"role\"] in [\"title\", \"sectionHeading\", \"pageHeader\"]:\n",
    "            string += \"<H1> \" + text_feilds[i][\"content\"] + \" </H1> \"\n",
    "\n",
    "        else:\n",
    "            string += text_feilds[i][\"content\"]\n",
    "\n",
    "        string += \"\\n\"\n",
    "\n",
    "    return string\n",
    "\n",
    "def get_docs(input_file_name, folder_path = \"\"):\n",
    "\n",
    "  support_doc_paths = \"{folder_path}/*.png\".format(folder_path = folder_path)\n",
    "  claim_doc_path = \"{folder_path}/{input_file_name}\".format(input_file_name = input_file_name, folder_path = folder_path)\n",
    "\n",
    "  claim_doc_txt, claim_doc_contents = process_files_in_parallel([claim_doc_path])\n",
    "\n",
    "  support_doc_paths = list(set(glob(support_doc_paths)) - set(glob(claim_doc_path)))\n",
    "  support_docs_txt, support_docs_contents = process_files_in_parallel(support_doc_paths)\n",
    "\n",
    "  return claim_doc_txt, support_docs_txt, support_docs_contents\n",
    "\n",
    "def get_df_string(temp_df):\n",
    "\n",
    "    temp_str_list = temp_df.to_csv(header=True, index=True).strip('\\n').split('\\n')\n",
    "\n",
    "    str_final = \"\"\n",
    "\n",
    "    for s in temp_str_list:\n",
    "        str_final += s\n",
    "        str_final += \"\\n\"\n",
    "\n",
    "    return str_final\n",
    "\n",
    "def filter_by_offset(text_feilds, tables):\n",
    "\n",
    "    output = []\n",
    "\n",
    "    text_offsets = []\n",
    "    for txt_dic in text_feilds:\n",
    "        text_offsets.append((txt_dic[\"spans\"][0][\"offset\"]))\n",
    "\n",
    "    prev_tab_end = -1\n",
    "\n",
    "    if len(tables) == 0:\n",
    "\n",
    "        texts_enclosed = []\n",
    "\n",
    "        str_text = get_text(text_feilds)\n",
    "\n",
    "        output.append(str_text)\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        for tab in tables:\n",
    "\n",
    "            spans = tab[\"spans\"]\n",
    "            spans_sorted = sorted(spans, key=lambda x: x['offset'])\n",
    "\n",
    "\n",
    "            for span in spans_sorted:\n",
    "                curr_tab_offset = span[\"offset\"]\n",
    "                curr_tab_end = curr_tab_offset + span[\"length\"]\n",
    "\n",
    "                idx_start_tab = binary_search(text_offsets, curr_tab_offset)\n",
    "                idx_end_tab = binary_search(text_offsets, curr_tab_end)\n",
    "\n",
    "                si = prev_tab_end + 1\n",
    "                ei = idx_start_tab\n",
    "\n",
    "                texts_enclosed = []\n",
    "\n",
    "                for i in range(si, ei):\n",
    "                    texts_enclosed.append(text_feilds[i])\n",
    "\n",
    "                str_text = get_text(texts_enclosed)\n",
    "\n",
    "                output.append(str_text)\n",
    "\n",
    "                prev_tab_end = idx_end_tab\n",
    "\n",
    "\n",
    "            output.append(get_table_df(tab))\n",
    "\n",
    "\n",
    "        global_max_ind = len(text_offsets)-1\n",
    "\n",
    "        if prev_tab_end == -1:\n",
    "\n",
    "            final_str = \"\"\n",
    "\n",
    "            for op_item in output:\n",
    "\n",
    "                if not isinstance(op_item, str):\n",
    "\n",
    "                    str_df = get_df_string(op_item)\n",
    "\n",
    "                    final_str += str_df\n",
    "\n",
    "\n",
    "                else:\n",
    "\n",
    "                    final_str += op_item\n",
    "\n",
    "                final_str += \"\\n\"\n",
    "\n",
    "            return final_str\n",
    "\n",
    "\n",
    "        si = prev_tab_end\n",
    "        ei = global_max_ind\n",
    "\n",
    "        texts_enclosed = []\n",
    "\n",
    "        for i in range(si, ei+1):\n",
    "            texts_enclosed.append(text_feilds[i])\n",
    "\n",
    "        str_text = get_text(texts_enclosed)\n",
    "\n",
    "        output.append(str_text)\n",
    "\n",
    "\n",
    "\n",
    "    final_str = \"\"\n",
    "\n",
    "    for op_item in output:\n",
    "\n",
    "        if not isinstance(op_item, str):\n",
    "\n",
    "            str_df = get_df_string(op_item)\n",
    "\n",
    "            final_str += str_df\n",
    "\n",
    "        else:\n",
    "\n",
    "            final_str += op_item\n",
    "\n",
    "        final_str += \"\\n\"\n",
    "\n",
    "    return final_str\n",
    "\n",
    "\n",
    "def extract_lines(ocr_result, page_num):\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    for idx, page in enumerate(ocr_result.pages):\n",
    "      width = ocr_result.pages[idx].width\n",
    "      height = ocr_result.pages[idx].height\n",
    "      for line in page.lines:\n",
    "        line_bbox = [(int(p.x), int(p.y)) for p in line.polygon]\n",
    "        start_x = line_bbox[0][0]*100/width\n",
    "        start_y = line_bbox[0][1]*100/height\n",
    "        end_x = line_bbox[1][0]*100/width\n",
    "        end_y = line_bbox[2][1]*100/height\n",
    "        lines.append((page_num, line.content, start_x, start_y, end_x, end_y, line.spans[0].offset, line.spans[0].length,width, height))\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def get_df_lines(text_lines):\n",
    "\n",
    "    df_result = pd.DataFrame(text_lines, columns=[\"page\", \"line\", \"line_start_x\", \n",
    "                                                          \"line_start_y\", \"line_end_x\", \"line_end_y\", \n",
    "                                                          \"offset\", \"length\", \"width\", \"height\"])\n",
    "\n",
    "    df_result = df_result.sort_values(by=[\"page\", \"line_start_y\", \"line_start_x\"])\n",
    "\n",
    "    df_result[\"line_no\"] = df_result[\"offset\"].rank()\n",
    "\n",
    "    df_result[\"content\"] = df_result.apply(lambda row: f'{row[\"page\"]}||{row[\"line_no\"]}||{row[\"line\"]}',\n",
    "                                                    axis=1)    \n",
    "    return df_result\n",
    "\n",
    "\n",
    "\n",
    "def get_outputs_processed(file_path, page_num):\n",
    "\n",
    "    with open(file_path, 'rb') as fopen:\n",
    "        file = fopen.read()\n",
    "\n",
    "    document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint = os.environ[\"AZURE_OCR_ENDPOINT\"], credential=AzureKeyCredential(os.environ[\"AZURE_OCR_KEY\"])\n",
    "    )\n",
    "\n",
    "    poller = document_analysis_client.begin_analyze_document(\"prebuilt-layout\", file)\n",
    "    result = poller.result()\n",
    "\n",
    "    resutls_dict = result.to_dict()\n",
    "\n",
    "    text_feilds = resutls_dict[\"paragraphs\"]\n",
    "    tables = resutls_dict[\"tables\"]\n",
    "\n",
    "    op = filter_by_offset(text_feilds, tables)\n",
    "\n",
    "    text_lines = extract_lines(result, page_num)\n",
    "\n",
    "    df_result = get_df_lines(text_lines)\n",
    "\n",
    "    return op, df_result\n",
    "\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"Key function for natural sorting.\"\"\"\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\\d+)', s)]\n",
    "\n",
    "def process_files_in_parallel(file_paths):\n",
    "    sorted_file_paths = sorted(file_paths, key=natural_sort_key)\n",
    "\n",
    "\n",
    "    page_mapping = {}\n",
    "    i = 0\n",
    "    for img_path in sorted_file_paths:\n",
    "        page_num = i\n",
    "        page_mapping[page_num] = img_path\n",
    "        i += 1\n",
    "\n",
    "    page_mapping = dict(sorted(page_mapping.items()))\n",
    "    print(\"page_mapping = \", page_mapping)\n",
    "    \n",
    "    # Use ThreadPoolExecutor to run get_outputs_processed in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_index = {executor.submit(get_outputs_processed, file, page_num): page_num for page_num, file in page_mapping.items()}\n",
    "        \n",
    "        results = [None] * len(sorted_file_paths)\n",
    "        for future in concurrent.futures.as_completed(future_to_index):\n",
    "            page_num = future_to_index[future]\n",
    "            try:\n",
    "                op, df_result = future.result()\n",
    "                results[page_num] = (op, df_result)\n",
    "            except Exception as exc:\n",
    "                print(f'File at index {page_num} generated an exception: {exc}')\n",
    "                results[page_num] = (None, pd.DataFrame())  # Maintain order by adding empty results in case of exception\n",
    "\n",
    "    ops = [res[0] for res in results]\n",
    "    dfs = [res[1] for res in results]\n",
    "\n",
    "    op_text = '\\n'.join(str(op) for op in ops if op is not None)\n",
    "\n",
    "    page_contents = []\n",
    "\n",
    "    for pgnum in range(len(dfs)):\n",
    "        width = dfs[pgnum].iloc[0][\"width\"]\n",
    "        height = dfs[pgnum].iloc[0][\"height\"]\n",
    "        page_contents.append({\"page_no\" : pgnum+1, \"df\": dfs[pgnum], \"content\":ops[pgnum], \"file_path\":sorted_file_paths[pgnum], \"width\":width, \"height\": height})\n",
    "\n",
    "    return op_text, page_contents\n",
    "\n",
    "def find_similar_lines_to_context(context_sentence, lines):\n",
    "    \n",
    "    '''Method to find similar lines to context in the chunk'''\n",
    "    scores = [fuzz.ratio(context_sentence.lower(),line.lower()) for line in lines]\n",
    "\n",
    "    sim_idx = np.argmax(scores)\n",
    "    max_score = max(scores)\n",
    "\n",
    "    min_idx = max(0, sim_idx)\n",
    "\n",
    "    while min_idx >= 0:\n",
    "        if scores[min_idx] > 60:\n",
    "            min_idx-=1\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "\n",
    "    max_idx = min(sim_idx+1, len(lines)-1)\n",
    "    while max_idx < len(scores):\n",
    "        if scores[max_idx] > 60:\n",
    "            max_idx+=1\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "\n",
    "    sim_lines = lines[min_idx: min(len(lines), max_idx+1)]\n",
    "\n",
    "    sim_page_lines = list(map(lambda x: x.split(\"||\")[:2], sim_lines))\n",
    "\n",
    "    return sim_page_lines, max_score\n",
    "\n",
    "\n",
    "def get_context_bbox(pages_df, context_sentence, lines):\n",
    "    '''Method to get context bbox'''\n",
    "\n",
    "    sim_page_lines, max_score = find_similar_lines_to_context(context_sentence, lines)\n",
    "\n",
    "    df_context = pd.DataFrame()\n",
    "    for page_ln in sim_page_lines:\n",
    "        df_temp = pages_df.loc[(pages_df[\"page\"] == float(page_ln[0])) & (pages_df[\"line_no\"] == float(page_ln[1]))]\n",
    "        df_context = pd.concat([df_context, df_temp])\n",
    "\n",
    "\n",
    "    if df_context.shape[0] > 0:\n",
    "        df_context[\"bbox\"] = df_context.apply(\n",
    "            lambda row: [\n",
    "                row[\"line_start_x\"],\n",
    "                row[\"line_start_y\"],\n",
    "                row[\"line_end_x\"],\n",
    "                row[\"line_start_y\"],\n",
    "                row[\"line_end_x\"],\n",
    "                row[\"line_end_y\"],\n",
    "                row[\"line_start_x\"],\n",
    "                row[\"line_end_y\"],\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "\n",
    "        df_context = df_context.sort_values(by=[\"page\", \"line_start_y\", \"line_start_x\"])\n",
    "        return (\n",
    "            df_context[\"bbox\"].values.tolist(),\n",
    "            df_context[\"page\"].tolist(),\n",
    "            df_context[\"line_no\"].tolist(),\n",
    "            max_score\n",
    "        )\n",
    "    return [[0] * 8], [0], [0], max_score\n",
    "\n",
    "\n",
    "def get_evidence_bbox(supporting_sentence, page_contents):\n",
    "\n",
    "    pages_bbox_scores = []\n",
    "\n",
    "    pages_bboxes = []\n",
    "\n",
    "    for i in range(len(page_contents)):\n",
    "\n",
    "        curr_pg_df = page_contents[i][\"df\"]\n",
    "\n",
    "        curr_pg_lines = page_contents[i][\"df\"][\"content\"]\n",
    "\n",
    "\n",
    "        bboxes_op = get_context_bbox(curr_pg_df, supporting_sentence, curr_pg_lines)\n",
    "\n",
    "        pages_bbox_scores.append(bboxes_op[-1])\n",
    "        pages_bboxes.append(bboxes_op[0])\n",
    "\n",
    "    max_pg = np.argmax(pages_bbox_scores)\n",
    "\n",
    "    max_page_contents = page_contents[max_pg]\n",
    "\n",
    "    file_path = max_page_contents[\"file_path\"]\n",
    "\n",
    "    width = max_page_contents[\"width\"]\n",
    "    height = max_page_contents[\"height\"]\n",
    "\n",
    "    max_bbox = pages_bboxes[max_pg]\n",
    "\n",
    "    print(\"pages_bbox_scores = \", pages_bbox_scores)\n",
    "    print(file_path)\n",
    "\n",
    "    return file_path, max_bbox, max_pg+1, width, height\n",
    "\n",
    "def get_response_claim(support_docs, claim_doc, user_prompt = 'here is a claim information where the claim for DME was denied because payer needed additional information: '):\n",
    "\n",
    "  prompt = user_prompt + claim_doc + ' Here are a set of supporting documents: ' + support_docs + ' Please give me the top 10 reasons (most important being the first and then the sorted rank order) why this claim should not be denied based on the supporting documents as a JSON. The JSON keys will be Reason, supporting_sentence, type_of_document for each of the 5 reasons. DONOT paraphrase or change the wording of the supporting sentences, return them as it is. Ensure that supporting_sentence contains the exact wordings from the supporting documents. The type of document can be any of '+'''Prescription/Order Certificate of Medical Necessity (CMN) Proof of Delivery (POD) Letter of Medical Necessity (LMN) Explanation of Benefits (EOB) Physician's Notes/Progress Reports Letter of Authorization (LOA) Invoice/Statement Referral Form ABN Test results'''\n",
    "  \n",
    "  return get_response(prompt)\n",
    "\n",
    "\n",
    "def convert_bbox_format(bbox):\n",
    "    # Unpack the bounding box coordinates\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = bbox\n",
    "    \n",
    "    # Calculate the top-left corner\n",
    "    x = x1\n",
    "    y = y1\n",
    "    \n",
    "    # Calculate width and height\n",
    "    width = x2 - x1\n",
    "    height = y4 - y1\n",
    "    \n",
    "    # Create the dictionary in the desired format\n",
    "    bbox_dict = {\n",
    "        \"x\": x,\n",
    "        \"y\": y,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    }\n",
    "    \n",
    "    return bbox_dict\n",
    "\n",
    "def get_bbox_label_studio(max_bbox):\n",
    "\n",
    "    new_bbox = []\n",
    "\n",
    "\n",
    "    for bbox in max_bbox:\n",
    "        converted_bbox = convert_bbox_format(bbox)\n",
    "        new_bbox.append(converted_bbox)\n",
    "\n",
    "    return new_bbox\n",
    "\n",
    "def get_appeals_evidence(reason_dict_list, support_docs_contents):\n",
    "\n",
    "    n = len(reason_dict_list)\n",
    "\n",
    "    evidences = []\n",
    "\n",
    "\n",
    "    for k in range(n):\n",
    "        supporting_sentence = reason_dict_list[k][\"supporting_sentence\"]\n",
    "        supporting_reason = reason_dict_list[k][\"Reason\"]\n",
    "\n",
    "\n",
    "        image_file_path, max_bbox, page_num, orig_width, orig_height = get_evidence_bbox(supporting_sentence, support_docs_contents)\n",
    "\n",
    "        bbox_final = get_bbox_label_studio(max_bbox)\n",
    "\n",
    "        evidences.append({\n",
    "            \"supporting_sentence\" : supporting_sentence,\n",
    "            \"supporting_reason\" : supporting_reason,\n",
    "            \"image_file_path\" : image_file_path,\n",
    "            \"bbox\" : bbox_final,\n",
    "            \"page_num\" : page_num,\n",
    "            \"orig_width\": orig_width, \n",
    "            \"orig_height\" : orig_height\n",
    "        })\n",
    "\n",
    "\n",
    "    return evidences\n",
    "\n",
    "\n",
    "def get_results_formatted(results):\n",
    "\n",
    "    op = []\n",
    "\n",
    "    file_paths = []\n",
    "    \n",
    "    i = 0\n",
    "    for res1 in results:\n",
    "\n",
    "        result = res1[\"res\"]\n",
    "        \n",
    "        res = {\n",
    "            \"id\" : \"result_\" + str(i+1),\n",
    "            \"original_width\": result[\"orig_width\"],\n",
    "            \"original_height\": result[\"orig_height\"],\n",
    "            \"image_rotation\": 0,\n",
    "            \"from_name\": \"label\",\n",
    "            \"to_name\": \"image\",\n",
    "            \"type\": \"rectanglelabels\",\n",
    "            \"value\": {\n",
    "                \"x\": result[\"x\"],\n",
    "                \"y\": result[\"y\"],\n",
    "                \"width\": result[\"width\"],\n",
    "                \"height\": result[\"height\"],\n",
    "                \"rotation\": 0,\n",
    "                \"rectanglelabels\": [\n",
    "                        res1[\"supporting_reason\"]\n",
    "                    ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        op.append(res)\n",
    "        file_path = res1[\"image_file_path\"]\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "    return (op,file_paths)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = os.environ[\"LABEL_STUDIO_ACCESS_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from s3_upload import create_presigned_post, generate_presigned_url, copy_file_to_s3\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file_to_s3(source_file_path, bucket_name, destination_path, expiration=900):\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        region_name=os.getenv(\"AWS_REGION_NAME\", \"us-east-2\"),\n",
    "    )\n",
    "    try:\n",
    "        s3_client.upload_file(source_file_path, bucket_name, destination_path)\n",
    "    except ClientError as e:\n",
    "        print(\"Error copying file to S3:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the reasons and supporting sentecnes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_mapping =  {0: 'Doc_imgs/new_files/tedlock/denial.png'}\n",
      "page_mapping =  {0: 'Doc_imgs/new_files/tedlock/1.png', 1: 'Doc_imgs/new_files/tedlock/2.png', 2: 'Doc_imgs/new_files/tedlock/3.png', 3: 'Doc_imgs/new_files/tedlock/4.png', 4: 'Doc_imgs/new_files/tedlock/5.png', 5: 'Doc_imgs/new_files/tedlock/6.png', 6: 'Doc_imgs/new_files/tedlock/7.png', 7: 'Doc_imgs/new_files/tedlock/8.png', 8: 'Doc_imgs/new_files/tedlock/9.png', 9: 'Doc_imgs/new_files/tedlock/10.png', 10: 'Doc_imgs/new_files/tedlock/11.png', 11: 'Doc_imgs/new_files/tedlock/12.png', 12: 'Doc_imgs/new_files/tedlock/13.png', 13: 'Doc_imgs/new_files/tedlock/14.png', 14: 'Doc_imgs/new_files/tedlock/15.png', 15: 'Doc_imgs/new_files/tedlock/16.png', 16: 'Doc_imgs/new_files/tedlock/17.png'}\n",
      "pages_bbox_scores =  [15, 25, 25, 25, 15, 31, 23, 21, 31, 26, 28, 30, 35, 26, 71, 19, 25]\n",
      "Doc_imgs/new_files/tedlock/15.png\n",
      "pages_bbox_scores =  [20, 30, 26, 26, 14, 26, 22, 24, 24, 23, 23, 20, 65, 28, 62, 24, 24]\n",
      "Doc_imgs/new_files/tedlock/13.png\n",
      "pages_bbox_scores =  [56, 48, 26, 28, 15, 29, 25, 23, 32, 32, 29, 48, 31, 48, 33, 24, 24]\n",
      "Doc_imgs/new_files/tedlock/1.png\n",
      "pages_bbox_scores =  [47, 88, 30, 33, 23, 29, 22, 23, 28, 28, 27, 31, 32, 32, 28, 24, 32]\n",
      "Doc_imgs/new_files/tedlock/2.png\n",
      "pages_bbox_scores =  [30, 30, 34, 20, 14, 30, 23, 30, 33, 26, 26, 40, 36, 40, 62, 17, 29]\n",
      "Doc_imgs/new_files/tedlock/15.png\n",
      "pages_bbox_scores =  [24, 25, 25, 28, 19, 27, 44, 49, 90, 35, 36, 37, 30, 32, 33, 23, 29]\n",
      "Doc_imgs/new_files/tedlock/9.png\n",
      "pages_bbox_scores =  [24, 24, 21, 30, 28, 29, 29, 20, 19, 21, 26, 25, 18, 19, 23, 21, 23]\n",
      "Doc_imgs/new_files/tedlock/4.png\n",
      "pages_bbox_scores =  [63, 63, 30, 28, 32, 31, 26, 26, 27, 25, 32, 27, 36, 34, 35, 53, 34]\n",
      "Doc_imgs/new_files/tedlock/1.png\n",
      "pages_bbox_scores =  [34, 34, 25, 22, 20, 23, 21, 23, 23, 24, 20, 21, 32, 24, 31, 49, 33]\n",
      "Doc_imgs/new_files/tedlock/16.png\n",
      "pages_bbox_scores =  [20, 30, 26, 26, 14, 26, 22, 24, 24, 23, 23, 20, 65, 28, 62, 24, 24]\n",
      "Doc_imgs/new_files/tedlock/13.png\n",
      "CPU times: user 4.12 s, sys: 212 ms, total: 4.34 s\n",
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# folder_path = \"Doc_imgs/new_files/betty\"\n",
    "# folder_path = \"Doc_imgs/new_files/gregory\"\n",
    "# folder_path = \"Doc_imgs/new_files/daniel\"\n",
    "# folder_path = \"Doc_imgs/new_files/osky\"\n",
    "# folder_path = \"Doc_imgs/new_files/april\"\n",
    "\n",
    "folder_path = \"Doc_imgs/new_files/tedlock\"\n",
    "\n",
    "\n",
    "folder_name = folder_path.split(\"/\")[-1]\n",
    "\n",
    "claim_doc_txt, support_docs_txt, support_docs_contents = get_docs(input_file_name = \"denial.png\",\n",
    "                                           folder_path=folder_path\n",
    "                                           )\n",
    "\n",
    "\n",
    "user_prompt = 'here is a claim information where the claim was denied because of lack of medical necessity, contract dispute & the HCPCS under contention are: A4223,B9999,A4245,A6219,A6457,A4927,S1015, A4209,E0776 NU '\n",
    "\n",
    "reason_dict = get_response_claim(support_docs=support_docs_txt, claim_doc=claim_doc_txt, user_prompt=user_prompt)\n",
    "\n",
    "reason_dict_list = reason_dict[list(reason_dict.keys())[0]]\n",
    "\n",
    "evidences = get_appeals_evidence(reason_dict_list, support_docs_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tedlock'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "results = []\n",
    "\n",
    "for evid in evidences:\n",
    "\n",
    "    res_id = i\n",
    "\n",
    "    curr_bbox = evid[\"bbox\"]\n",
    "\n",
    "    curr_oh = evid[\"orig_height\"]\n",
    "    curr_ow = evid[\"orig_width\"]\n",
    "\n",
    "\n",
    "    for bbox_dic in evid[\"bbox\"]:\n",
    "        temp_dict = {\n",
    "            \"res_id\" : res_id,\n",
    "            \"x\" : bbox_dic[\"x\"],\n",
    "            \"y\" : bbox_dic[\"y\"],\n",
    "            \"width\" : bbox_dic[\"width\"],\n",
    "            \"height\" : bbox_dic[\"height\"],\n",
    "            \"orig_height\" : curr_oh,\n",
    "            \"orig_width\" : curr_ow\n",
    "        }\n",
    "\n",
    "    \n",
    "        results.append({\n",
    "            \"supporting_sentence\" : evid[\"supporting_sentence\"],\n",
    "            \"supporting_reason\" : evid[\"supporting_reason\"],\n",
    "            \"image_file_path\" : evid[\"image_file_path\"], \n",
    "            \"page_num\" : evid[\"page_num\"], \n",
    "            \"res\" : temp_dict\n",
    "        })\n",
    "\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "appeal_reasons_list = []\n",
    "\n",
    "for res in results:\n",
    "    appeal_reasons_list.append(res[\"supporting_reason\"])\n",
    "\n",
    "appeal_reasons_set = set(appeal_reasons_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "appeal_reasons_set = list(appeal_reasons_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Physician's Recommendation\",\n",
       " 'Medical Necessity for Trilogy Humidifier',\n",
       " 'Proof of Delivery',\n",
       " 'Detailed Hospital Course',\n",
       " 'Medical Necessity for Non-Invasive Ventilator',\n",
       " 'Diagnosis Supporting Medical Necessity',\n",
       " 'Supporting Imaging Results',\n",
       " 'Supporting Lab Results',\n",
       " 'Prescription for Equipment',\n",
       " 'Length of Need']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appeal_reasons_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_color = [\n",
    "    (\"reason1\", \"#D9534F\"),  # darker shade of red\n",
    "    (\"reason2\", \"#E67E22\"),  # darker shade of orange\n",
    "    (\"reason3\", \"#F39C12\"),  # darker shade of yellow-orange\n",
    "    (\"reason4\", \"#F4D03F\"),  # darker shade of yellow\n",
    "    (\"reason5\", \"#5DADE2\"),  # darker shade of sky blue\n",
    "    (\"reason6\", \"#48C9B0\"),  # darker shade of teal\n",
    "    (\"reason7\", \"#1ABC9C\"),  # darker shade of medium turquoise\n",
    "    (\"reason8\", \"#16A085\"),  # darker shade of green-blue\n",
    "    (\"reason9\", \"#27AE60\"),  # darker shade of green\n",
    "    (\"reason10\", \"#2ECC71\"), # darker shade of emerald green\n",
    "    (\"reason11\", \"#2980B9\"), # darker shade of blue\n",
    "    (\"reason12\", \"#8E44AD\"), # darker shade of purple\n",
    "    (\"reason13\", \"#9B59B6\"), # darker shade of amethyst\n",
    "    (\"reason14\", \"#34495E\"), # darker shade of blue-gray\n",
    "    (\"reason15\", \"#2C3E50\"), # darker shade of midnight blue\n",
    "]\n",
    "\n",
    "reasons = []\n",
    "\n",
    "for idx,reason in enumerate(appeal_reasons_set):\n",
    "    reasons.append((reason, reason_color[idx][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_content = '''<View>\n",
    "  <Image name=\"image\" value=\"$image\"/>\n",
    "  <RectangleLabels name=\"label\" toName=\"image\">\n",
    "    {labels}\n",
    "  </RectangleLabels>\n",
    "</View>'''\n",
    "\n",
    "# Generate the Label elements dynamically\n",
    "label_elements = \"\"\n",
    "for reason, color in reasons:\n",
    "    label_element = f'<Label value=\"{reason}\" background=\"{color}\"/>'\n",
    "    label_elements += label_element + \"\\n    \"\n",
    "\n",
    "# Format the XML with the generated Label elements\n",
    "final_xml_content = xml_content.format(labels=label_elements.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<View>\n",
      "  <Image name=\"image\" value=\"$image\"/>\n",
      "  <RectangleLabels name=\"label\" toName=\"image\">\n",
      "    <Label value=\"Physician's Recommendation\" background=\"#D9534F\"/>\n",
      "    <Label value=\"Medical Necessity for Trilogy Humidifier\" background=\"#E67E22\"/>\n",
      "    <Label value=\"Proof of Delivery\" background=\"#F39C12\"/>\n",
      "    <Label value=\"Detailed Hospital Course\" background=\"#F4D03F\"/>\n",
      "    <Label value=\"Medical Necessity for Non-Invasive Ventilator\" background=\"#5DADE2\"/>\n",
      "    <Label value=\"Diagnosis Supporting Medical Necessity\" background=\"#48C9B0\"/>\n",
      "    <Label value=\"Supporting Imaging Results\" background=\"#1ABC9C\"/>\n",
      "    <Label value=\"Supporting Lab Results\" background=\"#16A085\"/>\n",
      "    <Label value=\"Prescription for Equipment\" background=\"#27AE60\"/>\n",
      "    <Label value=\"Length of Need\" background=\"#2ECC71\"/>\n",
      "  </RectangleLabels>\n",
      "</View>\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "print(final_xml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = \"0c5929f305c0d4987452bde11aabb7ed72381b88\"\n",
    "\n",
    "from label_studio_sdk.client import LabelStudio\n",
    "\n",
    "client = LabelStudio(\n",
    "    api_key=os.environ[\"LABEL_STUDIO_ACCESS_KEY\"],\n",
    "    base_url = os.environ[\"LABEL_STUDIO_BASE_URL\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the folder path\n",
    "\n",
    "# List to store relative file paths\n",
    "png_files = []\n",
    "png_filenames = []\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is a PNG and is not \"denial.png\"\n",
    "    if filename.endswith('.png') and filename != 'denial.png':\n",
    "        # Add the relative path to the list\n",
    "        png_files.append(os.path.join(folder_path, filename))\n",
    "        png_filenames.append(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_studio_sdk.client import LabelStudio\n",
    "\n",
    "\n",
    "# folder_name = \"betty\"\n",
    "\n",
    "resp_project = client.projects.create(title=folder_name, \n",
    "                     description= folder_name,\n",
    "                     label_config = final_xml_content\n",
    "                    )\n",
    "\n",
    "project_id = resp_project.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pushed :-  13.png\n",
      "pushed :-  16.png\n",
      "pushed :-  17.png\n",
      "pushed :-  15.png\n",
      "pushed :-  8.png\n",
      "pushed :-  11.png\n",
      "pushed :-  10.png\n",
      "pushed :-  4.png\n",
      "pushed :-  12.png\n",
      "pushed :-  9.png\n",
      "pushed :-  5.png\n",
      "pushed :-  14.png\n",
      "pushed :-  2.png\n",
      "pushed :-  7.png\n",
      "pushed :-  1.png\n",
      "pushed :-  6.png\n",
      "pushed :-  3.png\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "filename_task_mappings = []\n",
    "\n",
    "def process_file(i):\n",
    "    file_name = png_filenames[i]\n",
    "    file_path = png_files[i]\n",
    "    project_name = folder_name\n",
    "\n",
    "    copy_file_to_s3(\n",
    "        source_file_path=file_path,\n",
    "        bucket_name=os.environ[\"AWS_BUCKET_NAME\"],\n",
    "        destination_path=f\"{project_name}/{file_name}\"\n",
    "    )\n",
    "\n",
    "    print(\"pushed :- \", file_name)\n",
    "    filename_task_mappings.append((file_name, file_path))\n",
    "\n",
    "# Use ThreadPoolExecutor to run the tasks in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_file, i) for i in range(len(png_filenames))]\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            future.result()  # This ensures that any exceptions are raised\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage ID: 12\n"
     ]
    }
   ],
   "source": [
    "resp = client.import_storage.s3.create(use_blob_urls=True, \n",
    "                                project=project_id,\n",
    "                                prefix=project_name,\n",
    "                                bucket = os.environ[\"AWS_BUCKET_NAME\"],\n",
    "                                aws_access_key_id = os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "                                aws_secret_access_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "                                region_name = os.environ[\"AWS_REGION_NAME\"])\n",
    "storage_id = resp.id\n",
    "print(f\"Storage ID: {storage_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.05 ms, sys: 1.4 ms, total: 4.46 ms\n",
      "Wall time: 781 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# 3. sync storage\n",
    "\n",
    "resp = client.import_storage.s3.sync(\n",
    "    id=storage_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'completed'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_task_mappings = {}\n",
    "\n",
    "response = client.tasks.list(project=project_id)\n",
    "for item in response:\n",
    "\n",
    "    filename = item.storage_filename\n",
    "\n",
    "    fns = filename.split(\"/\")\n",
    "    key = fns[0] + \"_\" + fns[1]\n",
    "\n",
    "    filename_task_mappings[key] = item.id\n",
    "    # print(item.id)\n",
    "    # print(item.storage_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tedlock_1.png': 245,\n",
       " 'tedlock_10.png': 246,\n",
       " 'tedlock_11.png': 247,\n",
       " 'tedlock_12.png': 248,\n",
       " 'tedlock_13.png': 249,\n",
       " 'tedlock_14.png': 250,\n",
       " 'tedlock_15.png': 251,\n",
       " 'tedlock_16.png': 252,\n",
       " 'tedlock_17.png': 253,\n",
       " 'tedlock_2.png': 254,\n",
       " 'tedlock_3.png': 255,\n",
       " 'tedlock_4.png': 256,\n",
       " 'tedlock_5.png': 257,\n",
       " 'tedlock_6.png': 258,\n",
       " 'tedlock_7.png': 259,\n",
       " 'tedlock_8.png': 260,\n",
       " 'tedlock_9.png': 261}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_task_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = \"Doc_imgs/new_files/susan/9.png\"\n",
    "\n",
    "def get_task_mapping(filename_task_mappings, file_name):\n",
    "\n",
    "    key = \"_\".join(file_name.split(\"/\")[-2:])\n",
    "\n",
    "    return filename_task_mappings[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "results = []\n",
    "\n",
    "for evid in evidences:\n",
    "\n",
    "    res_id = i\n",
    "\n",
    "    curr_bbox = evid[\"bbox\"]\n",
    "\n",
    "    curr_oh = evid[\"orig_height\"]\n",
    "    curr_ow = evid[\"orig_width\"]\n",
    "\n",
    "\n",
    "    for bbox_dic in evid[\"bbox\"]:\n",
    "        temp_dict = {\n",
    "            \"res_id\" : res_id,\n",
    "            \"x\" : bbox_dic[\"x\"],\n",
    "            \"y\" : bbox_dic[\"y\"],\n",
    "            \"width\" : bbox_dic[\"width\"],\n",
    "            \"height\" : bbox_dic[\"height\"],\n",
    "            \"orig_height\" : curr_oh,\n",
    "            \"orig_width\" : curr_ow\n",
    "        }\n",
    "\n",
    "    \n",
    "        results.append({\n",
    "            \"supporting_sentence\" : evid[\"supporting_sentence\"],\n",
    "            \"supporting_reason\" : evid[\"supporting_reason\"],\n",
    "            \"image_file_path\" : evid[\"image_file_path\"], \n",
    "            \"page_num\" : evid[\"page_num\"], \n",
    "            \"res\" : temp_dict\n",
    "        })\n",
    "\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_sent_list = []\n",
    "\n",
    "for res in results:\n",
    "    supporting_sent_list.append(res[\"supporting_sentence\"])\n",
    "\n",
    "supporting_sent_set = set(supporting_sent_list)\n",
    "\n",
    "evidence_dict = dict()\n",
    "for res in results:\n",
    "\n",
    "    if res[\"supporting_sentence\"] in evidence_dict:\n",
    "        evidence_dict[res[\"supporting_sentence\"]].append(res)\n",
    "\n",
    "    else:\n",
    "        evidence_dict[res[\"supporting_sentence\"]] = [res]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_dict = {}\n",
    "\n",
    "for k in evidence_dict:\n",
    "\n",
    "    curr_results, file_paths = get_results_formatted(evidence_dict[k])\n",
    "\n",
    "    annotations_dict[k] = [curr_results, file_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "refined_annotations_dict = {}\n",
    "\n",
    "\n",
    "for k in annotations_dict:\n",
    "\n",
    "    file_paths = annotations_dict[k][1]\n",
    "\n",
    "    file_names = file_paths[0]\n",
    "\n",
    "\n",
    "    for idx , fp in enumerate(file_paths):\n",
    "        \n",
    "        key = \"_\".join(fp.split(\"/\")[-2:])\n",
    "\n",
    "        if key not in refined_annotations_dict:\n",
    "            refined_annotations_dict[key] = [annotations_dict[k][0][idx]]\n",
    "\n",
    "        else:\n",
    "\n",
    "            refined_annotations_dict[key].append(annotations_dict[k][0][idx])\n",
    "\n",
    "\n",
    "\n",
    "for k in refined_annotations_dict:\n",
    "\n",
    "    for idx, res in enumerate(refined_annotations_dict[k]):\n",
    "\n",
    "        res_id = \"result_\" + str(idx+1)\n",
    "\n",
    "        refined_annotations_dict[k][idx][\"id\"] = res_id\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tedlock_1.png': 245,\n",
       " 'tedlock_10.png': 246,\n",
       " 'tedlock_11.png': 247,\n",
       " 'tedlock_12.png': 248,\n",
       " 'tedlock_13.png': 249,\n",
       " 'tedlock_14.png': 250,\n",
       " 'tedlock_15.png': 251,\n",
       " 'tedlock_16.png': 252,\n",
       " 'tedlock_17.png': 253,\n",
       " 'tedlock_2.png': 254,\n",
       " 'tedlock_3.png': 255,\n",
       " 'tedlock_4.png': 256,\n",
       " 'tedlock_5.png': 257,\n",
       " 'tedlock_6.png': 258,\n",
       " 'tedlock_7.png': 259,\n",
       " 'tedlock_8.png': 260,\n",
       " 'tedlock_9.png': 261}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_task_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tedlock_15.png', 'tedlock_13.png', 'tedlock_1.png', 'tedlock_2.png', 'tedlock_9.png', 'tedlock_4.png', 'tedlock_16.png'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_annotations_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating for page :-\n",
      "tedlock_15.png\n",
      "\n",
      "Annotating for page :-\n",
      "tedlock_13.png\n",
      "\n",
      "Annotating for page :-\n",
      "tedlock_1.png\n",
      "\n",
      "Annotating for page :-\n",
      "tedlock_2.png\n",
      "\n",
      "Annotating for page :-\n",
      "tedlock_9.png\n",
      "\n",
      "Annotating for page :-\n",
      "tedlock_4.png\n",
      "\n",
      "Annotating for page :-\n",
      "tedlock_16.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in refined_annotations_dict:\n",
    "\n",
    "    print(\"Annotating for page :-\")\n",
    "    print(k)\n",
    "    print()\n",
    "\n",
    "    task_id = filename_task_mappings[k]\n",
    "\n",
    "\n",
    "    resp = client.annotations.create(\n",
    "    id = task_id,\n",
    "    project = project_id, \n",
    "    result=refined_annotations_dict[k],\n",
    "    was_cancelled=False,\n",
    "    ground_truth=True,\n",
    "    \n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
